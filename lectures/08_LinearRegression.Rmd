---
title: "Linear regression"
author: Shilin Zhao and Leena Choi
output:
  rmdformats::robobook:
    highlight: kate
    toc_depth: 3
    number_sections: no
    toc_float:
      collapsed: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
editor_options:
  markdown:
    wrap: 72
---

```{=html}
<style>
h2 {
  background-color: #D4DAEC;
}
body{
  font-size: 14pt;
}
</style>
```
## Introduction

-   Motivation

    -   Why we need linear regression?

-   Model assumptions

    -   **Linearity**
    -   **Independence**
    -   **Homoscedasticity**
    -   **Normality of residuals**

-   Perform simple and multiple linear regression analysis

-   Model diagnostics

## Motivation

These are **not just questions about correlation** — we want to
**model**, **predict**, and **understand** relationships between
variables.

Linear regression allows us to:

-    Quantify the **effect** of one variable (e.g., age) on another
    (e.g., blood pressure)

-   Make **predictions** for new individuals

-   Control for other variables (e.g., age *and* sex *and* BMI)

-   Evaluate **statistical significance** (is the effect real or due to
    chance?)

## Model assumptions

To make valid inferences from a linear regression (e.g., p-values,
confidence intervals), certain **assumptions must be met**. Let’s go
over each.

-   **Linearity**:

    The relationship between the independent variable(s) and the
    dependent variable is **linear**.

    **Check with a scatterplot**: If the pattern is curved or non-linear
    → linear regression isn't appropriate without transformation.

-   **Independence**: Each observation must be statistically independent
    of the others.

    No repeated measurements on the same subject (unless modeled
    correctly)

    No clusters or related subjects (e.g., twins, families) unless
    accounted for

-   **Homoscedasticity**: The variance of residuals (errors) should be
    the same across all levels of X.

    Plot residuals vs. fitted values.

-   **Normality of residuals:** The residuals (errors) should be
    approximately normally distributed.

    Plot a histogram or Q–Q plot of residuals

```{r,echo=FALSE}

set.seed(42)
x <- seq(0, 10, length.out = 100)

# 1. Linearity - good
y_linear <- 2 * x + rnorm(100, mean = 0, sd = 2)

# 2. Nonlinear pattern
y_nonlinear <- 2 * x + 5 * sin(x) + rnorm(100, mean = 0, sd = 2)

# 3. Homoscedasticity
y_homo <- 3 * x + rnorm(100, 0, 2)

# 4. Heteroscedasticity
y_hetero <- 3 * x + rnorm(100, 0, x)

# 5. Normal residuals
resid_normal <- rnorm(100)

# 6. Skewed residuals
resid_skewed <- rexp(100) - 1

par(mfrow = c(3, 2), mar = c(4, 4, 2, 1))

# 1. Good Linearity
plot(x, y_linear, main = "1. Good Linearity", pch = 19, col = "darkblue")
abline(lm(y_linear ~ x), col = "red")

# 2. Nonlinear Pattern
plot(x, y_nonlinear, main = "2. Nonlinear Pattern", pch = 19, col = "darkgreen")

# 3. Homoscedasticity
plot(x, residuals(lm(y_homo ~ x)), main = "3. Homoscedastic Residuals",
     pch = 19, col = "purple", ylab = "Residuals")
abline(h = 0, lty = 2, col = "red")

# 4. Heteroscedasticity
plot(x, residuals(lm(y_hetero ~ x)), main = "4. Heteroscedastic Residuals",
     pch = 19, col = "brown", ylab = "Residuals")
abline(h = 0, lty = 2, col = "red")

# 5. Normal Residuals
hist(resid_normal, breaks = 15, col = "lightblue", main = "5. Normal Residuals", xlab = "Residuals")
rug(resid_normal)

# 6. Skewed Residuals
hist(resid_skewed, breaks = 15, col = "lightpink", main = "6. Skewed Residuals", xlab = "Residuals")
rug(resid_skewed)

```

## Data example

We will use the same data, **fev**, used in the lecture
'06_TestsOnMeans'. This data have `fev` measuring lung function in
children who are smokers or non-smokers. Smoking tends to impair lung
function.

-   Important variables we use in this lecture:

    -   **sex**: subject gender (1 = male, 2 = female)

    -   **smoke**: smoke smoking habits (1 = yes, 2 = no)

```{r}

fev=read.csv("fev.csv",header=TRUE,row.names=1)
#fev <- read.csv("https://raw.githubusercontent.com/couthcommander/sip_computing/main/labs/fev.csv",header=TRUE,row.names=1)
fev$sex=ifelse(fev$sex==1,"male","female")
fev$smoke=ifelse(fev$smoke==1,"yes","no")

head(fev)

```

## Simple linear regression

**Question**: Does age associated with fev (lung function )?

### Checking assumptions

**Normality**: Does the dependent variable follows a normal
distribution?

```{r}

hist(fev$age, main="", xlab="Age")
hist(fev$fev, main="", xlab="FEV")

```

**Linearity**: Does the relationship between the independent variable
and the dependent variable is linear?

```{r}

plot(fev ~ age, data = fev, xlab="Age", ylab="FEV")

```

### Perform a linear regression analysis

Regress `fev` on `age`: $fev \sim b_0 + b_1 age$

```{r}

modelResult = lm(fev ~ age, data = fev)

summary(modelResult)


```

Interpretation of results:

-   The *Coefficients* are estimates of model parameters: the intercept
    ($b_0$) is 0.431648 and age ($b_1$) is 0.222041.

    -   The estimated regression equation can be written as fev =
        0.431648 + 0.222041\*age.
    -   The intercept ($b_0$) can be interpreted as the predicted fev
        value at age=0.
    -   The coefficient for age ($b_1$) is 0.222041; if age increases by
        one year, the predicted fev value increases by 0.222041.

-   The standard errors (SE) is the standard deviation of sampling
    distribution of the coefficients. The SE reflects how the
    coefficient would vary under repeated sampling.

-   The R-squared (R2) ranges from 0 to 1 and represents the proportion
    of variability in fev (dependent variable) explained by the model.
    For a simple linear regression, R2 is square of the Pearson's
    correlation coefficient.

-   The adjusted R-squared is a modified R-squared to adjust for the
    number of predictors in the model. It tries to correct for
    overestimation.

### Model diagnostics

#### Confidence interval of the estimates of model parameters

```{r}

confint(modelResult)

```

If the same study were to be repeated, there is a 95% chance that the
interval [0.21, 0.24] contains the true value of $b_1$.

#### Visualization of the fitted line

```{r}

plot(fev$age, fev$fev, main = "", xlab="Age", ylab="FEV")
#abline(modelResult, cex = 1.3, pch = 16, col=2)
lines(fev$age, modelResult$fitted.values, lty=1, col=2)
legend("topleft", "Fitted line", lty=1, col=2)
```

#### Checking the residuals

```{r}

par(mfrow=c(1, 2))
plot(modelResult$fitted.values, modelResult$residuals, main="Residuals vs. Fitted", xlab="Fitted values", ylab="Residuals")
abline(h=0, lty=2)

qqnorm(scale(modelResult$residuals))
abline(0,1,lty=2,col="red")

```

### Predict new data

```{r}

newData <- data.frame(age = c(Subject1 = 5, Subject2 = 10))
predict(modelResult, newData)


```

### Visualization of confidence interval

```{r}
preds <- predict(modelResult, data = fev, interval = 'confidence')






```

```{r}

head(preds)
```

```{r}
plot(fev$age,fev$fev,pch=16)

#add fitted regression line
abline(modelResult,col=2)
#add dashed lines for confidence bands
lines(fev$age, preds[ ,3], lty = 'dashed', col = 'blue')
lines(fev$age, preds[ ,2], lty = 'dashed', col = 'blue')
```

## Multiple linear regression

Multiple linear regression is an extension of simple linear regression
used to predict an outcome based on multiple predictors.

**Question**: Are age, gender, and smoking associated with fev (lung
function)?

### Data visualization

```{r}

boxplot(fev$fev~fev$smoke, main="", xlab="Smoke", ylab="FEV")

```

```{r}
ageInd=which(fev$age>=12)
length(ageInd)


```

```{r}
boxplot(fev$fev[ageInd]~fev$smoke[ageInd], main="Older kids only", xlab="Smoke", ylab="FEV")


```

```{r}
boxplot(fev$fev~fev$sex, main="", xlab="Gender", ylab="FEV")
```

### Perform a linear regression analysis

Regress `fev` on `smoke`: $fev \sim b_0 + b_1 smoke$

```{r}

fit1 <- lm(fev ~ smoke, data = fev)
summary(fit1)
```

Regress `fev` on `sex`: $fev \sim b_0 + b_1 sex$

```{r}
fit2 <- lm(fev ~ sex, data = fev)
summary(fit2)
```

### Visualization of age, gender, and smoking together

#### Age (by smoking) vs FEV

```{r}

plot(fev$age, fev$fev, main = "", xlab="Age", ylab="FEV",col=factor(fev$smoke),pch=16)
legend("topleft",legend=levels(factor(fev$smoke)),bty="n",col=1:2,pch=16)

```

#### Age (by gender) vs FEV

```{r}
plot(fev$age, fev$fev, main = "", xlab="Age", ylab="FEV",col=factor(fev$sex),pch=16)
legend("topleft",legend=levels(factor(fev$sex)),bty="n",col=1:2,pch=16)

```

### Perform a multiple linear regression analysis

Regress `fev` on `age` and `smoke`:
$fev \sim b_0 + b_1 age + b_2 smoke + b_3 sex$

```{r}

modelResult <- lm(fev ~ age + smoke+sex, data = fev)

summary(modelResult)

```

```{r}
confint(modelResult)
```

Interpretation of results:

-   The estimated regression line equation can be written as fev =
    0.24 + 0.23 \* age -0.15 \* Smoker +0.31 \* Male

-   How to interpret the coefficients?

```{r}
par(mfrow=c(1, 2))
plot(modelResult$fitted.values, modelResult$residuals, main="Residuals vs. Fitted", xlab="Fitted values", ylab="Residuals")
abline(h=0, lty=2)

qqnorm(scale(modelResult$residuals))
abline(0,1,lty=2,col="red")
```


```{r}

plot(fev$age, fev$fev, main = "", xlab="Age", ylab="FEV",col=factor(fev$smoke),pch=16)
legend("topleft",legend=levels(factor(fev$smoke)),bty="n",col=1:2,pch=16)
abline(modelResult$coefficients["(Intercept)"],modelResult$coefficients["age"],lwd=2)
abline(modelResult$coefficients["(Intercept)"]+modelResult$coefficients["smokeyes"],modelResult$coefficients["age"],col=2,lwd=2)
legend("topleft",legend=levels(factor(fev$smoke)),bty="n",col=1:2,pch=16)

```

```{r}

plot(fev$age, fev$fev, main = "", xlab="Age", ylab="FEV",col=factor(fev$sex),pch=16)
legend("topleft",legend=levels(factor(fev$sex)),bty="n",col=1:2,pch=16)
abline(modelResult$coefficients["(Intercept)"],modelResult$coefficients["age"],lwd=2)
abline(modelResult$coefficients["(Intercept)"]+modelResult$coefficients["sexmale"],modelResult$coefficients["age"],col=2,lwd=2)
legend("topleft",legend=levels(factor(fev$sex)),bty="n",col=1:2,pch=16)

```

### Effects of variables

```{r, message = FALSE}

library(effects)
plot(allEffects(modelResult))


```

### Multicollinearity?

Multicollinearity occurs when two or more predictor variables (X's) in a
regression model are highly correlated with each other. In other words,
One predictor can be linearly predicted from another, which means They
carry redundant information.

**Multicollinearity** **messes up interpretation and inference**.

-   Unstable coefficient estimates**:** Small changes in data → big
    changes in β^\hat{\beta}β\^​

```{=html}
<!-- -->
```
-   Inflated standard errors: Confidence intervals get wide; p-values
    become non-significant even when variables are important

-   Difficult to interpret effect sizes: You can’t separate the
    individual contribution of correlated variable

```{r}
# Simulate data
set.seed(123)
n <- 100

# Create a base variable (e.g., "lifestyle")
lifestyle <- rnorm(n, mean = 50, sd = 10)

# Create highly correlated variables
drinking <- lifestyle + rnorm(n, 0, 2)     # highly correlated with lifestyle
smoking  <- lifestyle + rnorm(n, 0, 2)     # also highly correlated

# Outcome depends on both
disease_risk <- 0.3 * drinking + 0.3 * smoking + rnorm(n, 0, 5)

# Fit multiple linear regression
model <- lm(disease_risk ~ drinking + smoking)

# View model summary
summary(model)
```

```{r}
cor(drinking, smoking)  # should be ~0.95 or higher
plot(drinking, smoking, main = "Drinking vs Smoking")
```

```{r}
# Model with only drinking
model_drink <- lm(disease_risk ~ drinking)
summary(model_drink)

# Model with only smoking
model_smoke <- lm(disease_risk ~ smoking)
summary(model_smoke)
```
